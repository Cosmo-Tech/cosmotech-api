spring:
  data:
    redis:
      host: "localhost"
      username: "default"
      # Leave it as blank as there's no auth with test container
      password:
      client-type: jedis
  main:
    banner-mode: "off"
  cloud:
    aws:
      credentials:
        access-key: "accessKeyId"
        secret-key: "secretAccessKey"
      s3:
        # We don't need/have a region for our local S3 service but the AWS SDKÂ requires one to be configured
        region: "region"
        # Enable path-style / disable DNS-style
        # By default, and for AWS S3, the client crafts its URL with the bucket as sub-domain of the endpoint
        # which is not how our current S3 implementation works as it expects the bucket in the path
        # '<bucket_name>.<endpoint>/<object_key>' DNS vs Path '<endpoint>/<bucket_name>/<object_key>'
        path-style-access-enabled: true
        endpoint: "http://localhost:9000"

management:
  endpoints:
    enabled-by-default: false

csm:
  platform:
    identityProvider:
      code: on_premise_one
      authorizationUrl: "http://fake_url:8080/authorize"
      tokenUrl: "http://fake_url:8080/token"
      containerScopes:
        '[https://api.cosmotech.com]': "Default scope"
      serverBaseUrl: "http://fake_url:8080"
      identity:
        clientId: "my_client_id"
        clientSecret: "my_client_secret"
        tenantId: "my_tenant_id"
      tls:
        enabled: false
        bundle: ""
    metrics:
      enabled: false
    api:
      # API Base Path for OpenAPI-generated controllers.
      # Might conflict with the SpringBoot context path, hence leaving it at the root
      base-path: /
      base-url: "http://fake_url:8080"
      version: latest
    id-generator:
      type: hashid
    event-publisher:
      type: in_process
    images:
      scenario-fetch-parameters: cosmo-tech/fetch-scenario-parameters
      send-datawarehouse: cosmo-tech/azure-data-explorer-connector
      scenario-data-upload: cosmo-tech/azure-storage-publish:latest
    containers:
      - name: "ADTTwingraphImport"
        imageRegistry: "ghcr.io"
        imageName: "cosmo-tech/adt-twincache-connector"
        imageVersion: "0.0.5"
      - name: "StorageTwingraphImport"
        imageRegistry: "ghcr.io"
        imageName: "cosmo-tech/azstorage-twincache-connector"
        imageVersion: "latest"
    upload:
      authorized-mime-types:
        workspaces:
          - application/zip
          - application/xml
          - application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
          - application/x-tika-ooxml
          - text/csv
          - text/plain
          - text/x-yaml
          - application/json
    authorization:
      tenant-id-jwt-claim: "iss"
      # Note that the way @Value works in Spring does not make it possible to inject this sole YAML list.
      # Use CsmPlatformProperties instead !
      allowed-tenants: ["test"]
    argo:
      base-uri: "https://argo-server.argo.svc.cluster.local:2746"
      workflows:
        namespace: phoenix
        node-pool-label: ""
        service-account-name: workflowcsmv2
        storage-class: null
        access-modes:
          # Any in the following list: ReadWriteOnce, ReadOnlyMany, ReadWriteMany, ReadWriteOncePod (K8s 1.22+)
          - ReadWriteOnce
        requests:
          storage: 1Gi
    twincache:
      host: "localhost"
      port: "6379"
      username: "default"
      password: "my-wonderful-password"
      useGraphModule: true
      tls:
        enabled: false
        bundle: ""
      runner:
        default-page-size: 20
    rbac:
      enabled: true
    s3:
      endpointUrl: "http://localhost:9000"
      bucketName: "test-bucket"
      accessKeyId: "s3_username"
      secretAccessKey: "s3_password"
